{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "efef47f9-ba19-4740-9c22-3f34cf762c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from transformers import \n",
    "\n",
    "import tensorflow as tf\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c571bc13-1b82-4d07-992f-1578131d11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../datasets/clean/final.npy\"\n",
    "data = np.load(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd22ce-58d2-468a-bdc7-a7c91ff55b14",
   "metadata": {},
   "source": [
    "- Import best model (trained on kaggle GPUs) to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "69bbf887-d8e7-45de-9a85-c96159f94cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    # text = text.replace(\" \", \"\")\n",
    "    \n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '<EMAIL>', text)\n",
    "    text = re.sub(r'\\b\\w+(?:\\.\\w+)+\\b', '<SUB>', text)\n",
    "    text = re.sub(r'(\\d+:)+\\d+', '<TIME>', text)\n",
    "    text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '<DATE>', text)\n",
    "    text = re.sub(r'\\b(\\w+)(?:,\\1)+\\b', '<SERIES>', text)\n",
    "    text = re.sub(r'\\b\\d+(?:,\\d+)+\\b', '<SERIES>', text)\n",
    "    text = re.sub(r'\\bchar\\(\\d+(?:\\+\\d+)*\\)', '<SERIES>', text)\n",
    "    text = re.sub(r'<SERIES>(?:\\+<SERIES>)+', '<SERIES>', text)\n",
    "    text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\b', '<NUMBER>', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', '<REPETITIVE>', text)\n",
    "    text = re.sub(r'(?<=[@$%^!~/[\\]\\\\` ])(?!a)\\w(?=[@$%^!~/[\\]\\\\` ])', '<SINGLE>', text)\n",
    "    \n",
    "    special_characters = r'[@$%^!~/[\\]\\-\\`]'\n",
    "    text = re.sub(rf'{special_characters}{{2,}}', '<REGEX>', text)\n",
    "    text = re.sub(special_characters, '<SPECIAL>', text)\n",
    "\n",
    "    text = text.replace(\",\", \"\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "09365acc-0e69-4039-956f-68ad935d7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def encode(query: str, tokenizer) -> list[int]:\n",
    "    query = pre_process(query)\n",
    "    tok = tokenizer.tokenize(query)\n",
    "    tok = list(map(lambda x: x.replace(\"Ġ\", \"\"), tok))\n",
    "    \n",
    "    for i, t in enumerate(tok):\n",
    "        if bool(re.fullmatch(r'[a-zA-Z]', t)) or bool(re.fullmatch(r'-?\\d+', t)):\n",
    "            tok[i] = \"<oov>\"\n",
    "            \n",
    "    seq = tokenizer.convert_tokens_to_ids(tok)\n",
    "    seq = [item for item in seq if item != None]\n",
    "    act_len = len(seq)\n",
    "    seq = pad_sequences(sequences=[seq], \n",
    "                              maxlen=15, \n",
    "                              padding=\"post\", \n",
    "                              truncating=\"post\")\n",
    "    return (seq, act_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5ceedfeb-6557-466a-80b7-587cfa334307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(sentence:str, tokenizer, model) -> float:\n",
    "    (sequence, act_len) = encode(sentence, tokenizer)\n",
    "    sentence = np.expand_dims(sentence, axis=0)\n",
    "    pred = model.predict(sequence)\n",
    "    seq_pred = np.argmax(pred, axis=2)\n",
    "    proba = np.sum(seq_pred[0, :act_len] == sequence[0, :act_len]) / act_len\n",
    "    \n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b77dd6be-e7f9-4867-8882-4d8ceeb18d7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPT2Tokenizer.__init__() missing 1 required positional argument: 'merges_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[228], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../utils/tokenier/vocab/vocab.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../utils/tokenier/vocab/merges.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: GPT2Tokenizer.__init__() missing 1 required positional argument: 'merges_file'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "vocab = \"../utils/tokenier/vocab/vocab.json\"\n",
    "merges = \"../utils/tokenier/vocab/merges.txt\"\n",
    "tokenizer = GPT2Tokenizer(vocab_file=vocab, merges_file=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8437ce05-2e9c-4e5e-ba70-71c13d2994d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../utils/best_models/tr_acc_8974_val_acc_8549.keras\"\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bc24d47d-b7c8-4a3c-ac10-f4aa1ec682b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "0.17857142857142858\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "0.21052631578947367\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0.14285714285714285\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "0.34615384615384615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "inp = [\n",
    "    \"' AND (SELECT COUNT(*) FROM information_schema.columns WHERE table_name='users') > 0 --\",\n",
    "    \"' AND (SELECT COUNT(*) FROM users) > 0 --\",\n",
    "    \"' AND 1=1 ORDER BY 2 --\",\n",
    "    \"' AND (SELECT SUBSTRING(password,1,1) FROM users WHERE username='admin')='h' --\",\n",
    "    \"SELECT DISTINCT country FROM customers ORDER BY country;\"\n",
    "]\n",
    "\n",
    "for q in inp:\n",
    "    print(predict_proba(q, tokenizer=tokenizer, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22753f-6084-4484-86ee-3781607ff406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
